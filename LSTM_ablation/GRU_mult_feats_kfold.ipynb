{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"e5e93d1c22834ef7a8debe704f0c18ab":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_4f0c46f1ee61417291df8225b5d57f26","IPY_MODEL_2147f7d7f1644f3281d41d64836f1d3e"],"layout":"IPY_MODEL_862c01e53c44484884028bbcc473df45"}},"4f0c46f1ee61417291df8225b5d57f26":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b72dbf0c0534edbb5086e65096921e0","placeholder":"​","style":"IPY_MODEL_14ffbf86eafa4c57a9121df150e8ba7b","value":"0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\r"}},"2147f7d7f1644f3281d41d64836f1d3e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_f442e88b54574c99825e5ac82e96ea9d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_48a685b2698540689d748a80cdd4ef72","value":1}},"862c01e53c44484884028bbcc473df45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b72dbf0c0534edbb5086e65096921e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14ffbf86eafa4c57a9121df150e8ba7b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f442e88b54574c99825e5ac82e96ea9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48a685b2698540689d748a80cdd4ef72":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip install yfinance\n","!pip install torchsummaryX\n","!pip install wandb -q"],"metadata":{"id":"gWN_FdML5lSP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671074653946,"user_tz":300,"elapsed":9281,"user":{"displayName":"Alex Wang","userId":"17805231019310844218"}},"outputId":"faf4efad-914c-4e5a-d04e-b0532d4e4bf0"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: yfinance in /usr/local/lib/python3.8/dist-packages (0.1.90)\n","Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.8/dist-packages (from yfinance) (1.4.4)\n","Requirement already satisfied: requests>=2.26 in /usr/local/lib/python3.8/dist-packages (from yfinance) (2.28.1)\n","Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.8/dist-packages (from yfinance) (0.0.11)\n","Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from yfinance) (1.3.5)\n","Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from yfinance) (1.21.6)\n","Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.8/dist-packages (from yfinance) (4.9.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.0->yfinance) (2022.6)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.3.0->yfinance) (1.15.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26->yfinance) (1.26.13)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26->yfinance) (2022.9.24)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26->yfinance) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26->yfinance) (2.10)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchsummaryX in /usr/local/lib/python3.8/dist-packages (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchsummaryX) (1.21.6)\n","Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from torchsummaryX) (1.13.0+cu116)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from torchsummaryX) (1.3.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->torchsummaryX) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->torchsummaryX) (2022.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->torchsummaryX) (1.15.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->torchsummaryX) (4.4.0)\n"]}]},{"cell_type":"code","execution_count":53,"metadata":{"id":"mAGT7V2e5LIJ","executionInfo":{"status":"ok","timestamp":1671074653947,"user_tz":300,"elapsed":10,"user":{"displayName":"Alex Wang","userId":"17805231019310844218"}}},"outputs":[],"source":["%matplotlib inline\n","\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import time\n","import os\n","import yfinance as yf\n","import pandas as pd\n","import datetime\n","import torch\n","import torch.nn as nn\n","from torchsummaryX import summary\n","from torch.autograd import Variable\n","from sklearn.preprocessing import MinMaxScaler\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm\n","import gc\n","from sklearn.model_selection import KFold\n","from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","os.getcwd()"],"metadata":{"id":"2VCiYec-5Q8Q","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1671074654926,"user_tz":300,"elapsed":760,"user":{"displayName":"Alex Wang","userId":"17805231019310844218"}},"outputId":"fbdf3d5e-7820-4f3c-83c2-d0f552c5242f"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["import wandb\n","wandb.login(key=\"03916b709446813b51f72a1b29a2854a9dd9e3f7\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z2IEMnzk3TGi","executionInfo":{"status":"ok","timestamp":1671074654926,"user_tz":300,"elapsed":10,"user":{"displayName":"Alex Wang","userId":"17805231019310844218"}},"outputId":"0508d378-4581-412a-e026-ddf333896b6b"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["# Training Config\n","config = {\n","    \"batch_size\" : 32,\n","    \"lr\" : 1e-3,\n","    \"epochs\" : 100,\n","    \"look_back\" : 60,\n","    \"feature_dim\" : 6,\n","    \"hidden_dim\" : 64,\n","    \"output_dim\" : 1,\n","    \"num_layers\" : 3,\n","    \"dropout\" : 0.5\n","}"],"metadata":{"id":"MMr7OA95OPgj","executionInfo":{"status":"ok","timestamp":1671074654926,"user_tz":300,"elapsed":6,"user":{"displayName":"Alex Wang","userId":"17805231019310844218"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["pd.set_option('display.max_rows', 1000)\n","pd.set_option('display.max_columns', 100)\n","pd.set_option('display.width', 200)\n","\n","class VolatiltiyData:\n","\n","    def __init__(self, tickers, start_year, start_month, start_date, end_year,\n","                 end_month, end_date, freq, scaling_factor, windows, y_window):\n","        self.tickers = tickers\n","        self.start = datetime.datetime(start_year, start_month, start_date)\n","        self.end = datetime.datetime(end_year, end_month, end_date)\n","        self.freq = freq\n","        self.scaling_factor = scaling_factor\n","        # 过去几天的vol\n","        self.windows = windows\n","        # Predict几天的vol\n","        self.y_window = y_window \n","\n","    def get_data(self):\n","        \"\"\"\n","        Output:\n","        raw data -> ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', \n","                     'Log Adj Close', 'Log Return', 'Log Volume', 'Log Volume Chg', 'Log Range', '10-day-vol']\n","        \"\"\"\n","        data = yf.download(self.tickers, start = self.start, end = self.end, interval = self.freq)\n","        data[\"Log Adj Close\"] = np.log(data[\"Adj Close\"])\n","        data[\"Log Return\"] = np.insert(np.diff(data[\"Log Adj Close\"]), 0, 0) * self.scaling_factor\n","        data[\"Log Volume\"] = np.log(data[\"Volume\"])\n","        data[\"Log Volume Chg\"] = np.insert(np.diff(data[\"Log Volume\"]), 0, 0)\n","        data[\"Log Range\"] = np.log(data[\"High\"] / data[\"Low\"]) * self.scaling_factor\n","        data[\"10-day-vol\"] = data[\"Log Return\"].rolling(10).std(ddof=0)\n","        data[\"30-day-vol\"] = data[\"Log Return\"].rolling(30).std(ddof=0)\n","\n","        return data\n","\n","    def get_vix_data(self):\n","        \"\"\"\n","        Output:\n","        raw vix data -> ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n","        \"\"\"\n","        data = yf.download(\"^VIX\", start = self.start, end = self.end, interval = self.freq)\n","        \n","        return data\n","\n","    def prepare_data(self, *args):\n","        \"\"\"\n","        Prepare data for training. Select features that are needed. Perform necessary normalization.\n","        \n","        For volatiltiy data, we need to remove initial n days since we need at least n days to calculate volatiltiy.\n","\n","        Input:\n","        Multiple types of data:\n","        Basic data: 10-day-vol, Log Volume Chg, Log Range\n","        Optional data: VIX?\n","\n","        Output:\n","        dataset ->  (samples, features)\n","        scalar  ->  scalar for our normalization\n","\n","        Current features:\n","        [10-day-vol, Log Return, Log Volume Chg, Log Range, 30-day-vol, VIX]\n","        Update based on correlation [10-day-vol, Log Range, 30-day-vol, VIX]\n","        \"\"\"\n","        data = args[0]\n","        vol_10 = data['10-day-vol'].values.reshape(-1, 1)\n","        log_return = data['Log Return'].values.reshape(-1, 1)\n","        log_vlmchg = data['Log Volume Chg'].values.reshape(-1, 1)\n","        log_range = data['Log Range'].values.reshape(-1, 1)\n","        vol_30 = data['30-day-vol'].values.reshape(-1, 1)\n","        \n","        dataset = vol_10\n","        dataset = np.append(dataset, log_return, axis=1)\n","        dataset = np.append(dataset, log_vlmchg, axis = 1)\n","        dataset = np.append(dataset, log_range, axis = 1)\n","        dataset = np.append(dataset, vol_30, axis = 1)\n","        \n","        # For appending more types of data -> VIX\n","        for i in range(1, len(args)):\n","            extra_data = args[i]['Adj Close'].values.reshape(-1, 1)\n","            dataset = np.append(dataset, extra_data, axis = 1)\n","\n","        dataset = dataset[31:]\n","        \n","        # normalize the dataset\n","        scaler = MinMaxScaler(feature_range=(0, 1))\n","        dataset = scaler.fit_transform(dataset)\n","        return dataset, vol_10, scaler\n","\n","volatiltiy_data = VolatiltiyData(\n","                      \"^GSPC\",\n","                      1990, 1, 2,\n","                      2022, 12, 12,\n","                      freq=\"1d\",\n","                      scaling_factor=100,\n","                      windows=[10], # 过去几天的vol\n","                      y_window=10 # Predict几天的vol\n","                  )\n","\n","data = volatiltiy_data.get_data()\n","# print(np.shape(data))\n","# print(data)\n","# print(\"#######################################################################\")\n","vix_data = volatiltiy_data.get_vix_data()\n","# print(np.shape(vix_data))\n","# print(vix_data)\n","# print(\"#######################################################################\")\n","dataset, vol_10, scaler = volatiltiy_data.prepare_data(data, vix_data)\n","print(np.shape(dataset))\n","# print(dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qjJR70yNR4EE","executionInfo":{"status":"ok","timestamp":1671074655490,"user_tz":300,"elapsed":569,"user":{"displayName":"Alex Wang","userId":"17805231019310844218"}},"outputId":"c66f3eaf-782c-44c6-ee28-eec54e7d7a07"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","(8270, 6)\n"]}]},{"cell_type":"code","source":["df = data.copy()\n","\n","df['VIX'] = vix_data['Adj Close']\n","df['Y'] = list(df['10-day-vol'])[1:] + [0]\n","df.iloc[31:, :].corr()"],"metadata":{"id":"cqsmnoGtyx47","colab":{"base_uri":"https://localhost:8080/","height":598},"executionInfo":{"status":"ok","timestamp":1671074655704,"user_tz":300,"elapsed":216,"user":{"displayName":"Alex Wang","userId":"17805231019310844218"}},"outputId":"f6604349-97f0-465a-b078-c1453952862d"},"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                    Open      High       Low     Close  Adj Close    Volume  Log Adj Close  Log Return  Log Volume  Log Volume Chg  Log Range  10-day-vol  30-day-vol       VIX         Y\n","Open            1.000000  0.999939  0.999908  0.999841   0.999841  0.606128       0.928038   -0.007309    0.662764       -0.000161  -0.030450    0.025482    0.039784  0.005281  0.025495\n","High            0.999939  1.000000  0.999878  0.999916   0.999916  0.607602       0.928124   -0.000608    0.663549        0.000218  -0.025355    0.030240    0.043871  0.008986  0.030176\n","Low             0.999908  0.999878  1.000000  0.999924   0.999924  0.603854       0.928091    0.001205    0.661675       -0.001773  -0.038569    0.020281    0.035356 -0.000993  0.020006\n","Close           0.999841  0.999916  0.999924  1.000000   1.000000  0.605676       0.928174    0.007725    0.662611       -0.001164  -0.032200    0.025501    0.039853  0.003284  0.025198\n","Adj Close       0.999841  0.999916  0.999924  1.000000   1.000000  0.605676       0.928174    0.007725    0.662611       -0.001164  -0.032200    0.025501    0.039853  0.003284  0.025198\n","Volume          0.606128  0.607602  0.603854  0.605676   0.605676  1.000000       0.676620   -0.025952    0.909445        0.107652   0.287947    0.334698    0.339552  0.312287  0.339322\n","Log Adj Close   0.928038  0.928124  0.928091  0.928174   0.928174  0.676620       1.000000    0.006101    0.823790       -0.001052   0.007233    0.063723    0.080503  0.022871  0.063375\n","Log Return     -0.007309 -0.000608  0.001205  0.007725   0.007725 -0.025952       0.006101    1.000000   -0.012735       -0.028871  -0.104016   -0.005016   -0.001999 -0.134462 -0.028897\n","Log Volume      0.662764  0.663549  0.661675  0.662611   0.662611  0.909445       0.823790   -0.012735    1.000000        0.084669   0.196749    0.240805    0.255808  0.200707  0.242190\n","Log Volume Chg -0.000161  0.000218 -0.001773 -0.001164  -0.001164  0.107652      -0.001052   -0.028871    0.084669        1.000000   0.160535   -0.009776   -0.008562  0.009929 -0.009615\n","Log Range      -0.030450 -0.025355 -0.038569 -0.032200  -0.032200  0.287947       0.007233   -0.104016    0.196749        0.160535   1.000000    0.738532    0.659856  0.760296  0.749729\n","10-day-vol      0.025482  0.030240  0.020281  0.025501   0.025501  0.334698       0.063723   -0.005016    0.240805       -0.009776   0.738532    1.000000    0.872313  0.851580  0.978613\n","30-day-vol      0.039784  0.043871  0.035356  0.039853   0.039853  0.339552       0.080503   -0.001999    0.255808       -0.008562   0.659856    0.872313    1.000000  0.868599  0.854883\n","VIX             0.005281  0.008986 -0.000993  0.003284   0.003284  0.312287       0.022871   -0.134462    0.200707        0.009929   0.760296    0.851580    0.868599  1.000000  0.856794\n","Y               0.025495  0.030176  0.020006  0.025198   0.025198  0.339322       0.063375   -0.028897    0.242190       -0.009615   0.749729    0.978613    0.854883  0.856794  1.000000"],"text/html":["\n","  <div id=\"df-956fe027-444a-4662-9e65-de3fabc241a7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Open</th>\n","      <th>High</th>\n","      <th>Low</th>\n","      <th>Close</th>\n","      <th>Adj Close</th>\n","      <th>Volume</th>\n","      <th>Log Adj Close</th>\n","      <th>Log Return</th>\n","      <th>Log Volume</th>\n","      <th>Log Volume Chg</th>\n","      <th>Log Range</th>\n","      <th>10-day-vol</th>\n","      <th>30-day-vol</th>\n","      <th>VIX</th>\n","      <th>Y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Open</th>\n","      <td>1.000000</td>\n","      <td>0.999939</td>\n","      <td>0.999908</td>\n","      <td>0.999841</td>\n","      <td>0.999841</td>\n","      <td>0.606128</td>\n","      <td>0.928038</td>\n","      <td>-0.007309</td>\n","      <td>0.662764</td>\n","      <td>-0.000161</td>\n","      <td>-0.030450</td>\n","      <td>0.025482</td>\n","      <td>0.039784</td>\n","      <td>0.005281</td>\n","      <td>0.025495</td>\n","    </tr>\n","    <tr>\n","      <th>High</th>\n","      <td>0.999939</td>\n","      <td>1.000000</td>\n","      <td>0.999878</td>\n","      <td>0.999916</td>\n","      <td>0.999916</td>\n","      <td>0.607602</td>\n","      <td>0.928124</td>\n","      <td>-0.000608</td>\n","      <td>0.663549</td>\n","      <td>0.000218</td>\n","      <td>-0.025355</td>\n","      <td>0.030240</td>\n","      <td>0.043871</td>\n","      <td>0.008986</td>\n","      <td>0.030176</td>\n","    </tr>\n","    <tr>\n","      <th>Low</th>\n","      <td>0.999908</td>\n","      <td>0.999878</td>\n","      <td>1.000000</td>\n","      <td>0.999924</td>\n","      <td>0.999924</td>\n","      <td>0.603854</td>\n","      <td>0.928091</td>\n","      <td>0.001205</td>\n","      <td>0.661675</td>\n","      <td>-0.001773</td>\n","      <td>-0.038569</td>\n","      <td>0.020281</td>\n","      <td>0.035356</td>\n","      <td>-0.000993</td>\n","      <td>0.020006</td>\n","    </tr>\n","    <tr>\n","      <th>Close</th>\n","      <td>0.999841</td>\n","      <td>0.999916</td>\n","      <td>0.999924</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.605676</td>\n","      <td>0.928174</td>\n","      <td>0.007725</td>\n","      <td>0.662611</td>\n","      <td>-0.001164</td>\n","      <td>-0.032200</td>\n","      <td>0.025501</td>\n","      <td>0.039853</td>\n","      <td>0.003284</td>\n","      <td>0.025198</td>\n","    </tr>\n","    <tr>\n","      <th>Adj Close</th>\n","      <td>0.999841</td>\n","      <td>0.999916</td>\n","      <td>0.999924</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.605676</td>\n","      <td>0.928174</td>\n","      <td>0.007725</td>\n","      <td>0.662611</td>\n","      <td>-0.001164</td>\n","      <td>-0.032200</td>\n","      <td>0.025501</td>\n","      <td>0.039853</td>\n","      <td>0.003284</td>\n","      <td>0.025198</td>\n","    </tr>\n","    <tr>\n","      <th>Volume</th>\n","      <td>0.606128</td>\n","      <td>0.607602</td>\n","      <td>0.603854</td>\n","      <td>0.605676</td>\n","      <td>0.605676</td>\n","      <td>1.000000</td>\n","      <td>0.676620</td>\n","      <td>-0.025952</td>\n","      <td>0.909445</td>\n","      <td>0.107652</td>\n","      <td>0.287947</td>\n","      <td>0.334698</td>\n","      <td>0.339552</td>\n","      <td>0.312287</td>\n","      <td>0.339322</td>\n","    </tr>\n","    <tr>\n","      <th>Log Adj Close</th>\n","      <td>0.928038</td>\n","      <td>0.928124</td>\n","      <td>0.928091</td>\n","      <td>0.928174</td>\n","      <td>0.928174</td>\n","      <td>0.676620</td>\n","      <td>1.000000</td>\n","      <td>0.006101</td>\n","      <td>0.823790</td>\n","      <td>-0.001052</td>\n","      <td>0.007233</td>\n","      <td>0.063723</td>\n","      <td>0.080503</td>\n","      <td>0.022871</td>\n","      <td>0.063375</td>\n","    </tr>\n","    <tr>\n","      <th>Log Return</th>\n","      <td>-0.007309</td>\n","      <td>-0.000608</td>\n","      <td>0.001205</td>\n","      <td>0.007725</td>\n","      <td>0.007725</td>\n","      <td>-0.025952</td>\n","      <td>0.006101</td>\n","      <td>1.000000</td>\n","      <td>-0.012735</td>\n","      <td>-0.028871</td>\n","      <td>-0.104016</td>\n","      <td>-0.005016</td>\n","      <td>-0.001999</td>\n","      <td>-0.134462</td>\n","      <td>-0.028897</td>\n","    </tr>\n","    <tr>\n","      <th>Log Volume</th>\n","      <td>0.662764</td>\n","      <td>0.663549</td>\n","      <td>0.661675</td>\n","      <td>0.662611</td>\n","      <td>0.662611</td>\n","      <td>0.909445</td>\n","      <td>0.823790</td>\n","      <td>-0.012735</td>\n","      <td>1.000000</td>\n","      <td>0.084669</td>\n","      <td>0.196749</td>\n","      <td>0.240805</td>\n","      <td>0.255808</td>\n","      <td>0.200707</td>\n","      <td>0.242190</td>\n","    </tr>\n","    <tr>\n","      <th>Log Volume Chg</th>\n","      <td>-0.000161</td>\n","      <td>0.000218</td>\n","      <td>-0.001773</td>\n","      <td>-0.001164</td>\n","      <td>-0.001164</td>\n","      <td>0.107652</td>\n","      <td>-0.001052</td>\n","      <td>-0.028871</td>\n","      <td>0.084669</td>\n","      <td>1.000000</td>\n","      <td>0.160535</td>\n","      <td>-0.009776</td>\n","      <td>-0.008562</td>\n","      <td>0.009929</td>\n","      <td>-0.009615</td>\n","    </tr>\n","    <tr>\n","      <th>Log Range</th>\n","      <td>-0.030450</td>\n","      <td>-0.025355</td>\n","      <td>-0.038569</td>\n","      <td>-0.032200</td>\n","      <td>-0.032200</td>\n","      <td>0.287947</td>\n","      <td>0.007233</td>\n","      <td>-0.104016</td>\n","      <td>0.196749</td>\n","      <td>0.160535</td>\n","      <td>1.000000</td>\n","      <td>0.738532</td>\n","      <td>0.659856</td>\n","      <td>0.760296</td>\n","      <td>0.749729</td>\n","    </tr>\n","    <tr>\n","      <th>10-day-vol</th>\n","      <td>0.025482</td>\n","      <td>0.030240</td>\n","      <td>0.020281</td>\n","      <td>0.025501</td>\n","      <td>0.025501</td>\n","      <td>0.334698</td>\n","      <td>0.063723</td>\n","      <td>-0.005016</td>\n","      <td>0.240805</td>\n","      <td>-0.009776</td>\n","      <td>0.738532</td>\n","      <td>1.000000</td>\n","      <td>0.872313</td>\n","      <td>0.851580</td>\n","      <td>0.978613</td>\n","    </tr>\n","    <tr>\n","      <th>30-day-vol</th>\n","      <td>0.039784</td>\n","      <td>0.043871</td>\n","      <td>0.035356</td>\n","      <td>0.039853</td>\n","      <td>0.039853</td>\n","      <td>0.339552</td>\n","      <td>0.080503</td>\n","      <td>-0.001999</td>\n","      <td>0.255808</td>\n","      <td>-0.008562</td>\n","      <td>0.659856</td>\n","      <td>0.872313</td>\n","      <td>1.000000</td>\n","      <td>0.868599</td>\n","      <td>0.854883</td>\n","    </tr>\n","    <tr>\n","      <th>VIX</th>\n","      <td>0.005281</td>\n","      <td>0.008986</td>\n","      <td>-0.000993</td>\n","      <td>0.003284</td>\n","      <td>0.003284</td>\n","      <td>0.312287</td>\n","      <td>0.022871</td>\n","      <td>-0.134462</td>\n","      <td>0.200707</td>\n","      <td>0.009929</td>\n","      <td>0.760296</td>\n","      <td>0.851580</td>\n","      <td>0.868599</td>\n","      <td>1.000000</td>\n","      <td>0.856794</td>\n","    </tr>\n","    <tr>\n","      <th>Y</th>\n","      <td>0.025495</td>\n","      <td>0.030176</td>\n","      <td>0.020006</td>\n","      <td>0.025198</td>\n","      <td>0.025198</td>\n","      <td>0.339322</td>\n","      <td>0.063375</td>\n","      <td>-0.028897</td>\n","      <td>0.242190</td>\n","      <td>-0.009615</td>\n","      <td>0.749729</td>\n","      <td>0.978613</td>\n","      <td>0.854883</td>\n","      <td>0.856794</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-956fe027-444a-4662-9e65-de3fabc241a7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-956fe027-444a-4662-9e65-de3fabc241a7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-956fe027-444a-4662-9e65-de3fabc241a7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["class DataLoaderForVolatilityModeling(DataLoader):\n","    \"\"\"\n","    Construct dataloader\n","  \n","    The label for our data is the next 10-day volatility after our input.\n","\n","    Output:\n","    input -> (batch_size, seq_len, feature_size)\n","    label -> (batch_size, 1, feature_size) \n","    \"\"\"\n","    def __init__(self, dataset, vol_10, batch_size, sequence_length, shuffle=True, feature_size=1):\n","        self.dataset = dataset\n","        self.vol_10 = vol_10\n","        self.batch_size = batch_size\n","        self.shuffle = shuffle\n","        self.sequence_length = sequence_length\n","        self.num_batches = (np.shape(self.dataset)[0]-10) // self.batch_size\n","        self.feature_size = feature_size\n","\n","    def __len__(self):\n","        return self.num_batches - self.sequence_length\n","\n","    def __iter__(self):\n","        # group the sequence into batches\n","        x = torch.from_numpy(np.reshape(self.dataset[:self.num_batches*self.batch_size, :], (self.batch_size, self.num_batches, config['feature_dim']))).type(torch.float32)\n","        y = torch.from_numpy(np.reshape(self.vol_10[10:self.num_batches*self.batch_size+10, :], (self.batch_size, -1))).type(torch.float32)\n","\n","        # return a tuple of (input, label) on every iteration with yield\n","        index = 0\n","        while index+self.sequence_length < self.num_batches:\n","            time_steps = self.sequence_length\n","            input = x[:, index:index+time_steps, :]\n","            label = y[:, index+time_steps].view(self.batch_size, 1)\n","            index += 1\n","            yield input, label"],"metadata":{"id":"dMj8hQ_Q6ISO","executionInfo":{"status":"ok","timestamp":1671074655704,"user_tz":300,"elapsed":4,"user":{"displayName":"Alex Wang","userId":"17805231019310844218"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["# sanity check\n","dataloader = DataLoaderForVolatilityModeling(dataset, vol_10, batch_size=config[\"batch_size\"], sequence_length=config[\"look_back\"], feature_size=config[\"feature_dim\"])\n","tmp = iter(dataloader)\n","input, label = next(tmp) \n","print(f'input shape = {np.shape(input)}')\n","print(f'label shape = {np.shape(label)}')"],"metadata":{"id":"55ksKW4j-1q6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671074655704,"user_tz":300,"elapsed":4,"user":{"displayName":"Alex Wang","userId":"17805231019310844218"}},"outputId":"4fc2280d-aeb6-4a86-99d3-22853a574d07"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["input shape = torch.Size([32, 60, 6])\n","label shape = torch.Size([32, 1])\n"]}]},{"cell_type":"code","source":["# model\n","class Model(nn.Module):\n","    \"\"\"\n","    Construct model architecture\n","    \"\"\"\n","    def __init__(self, feature_dim, hidden_dim, output_dim):\n","        super(Model, self).__init__()\n","        self.lstm = torch.nn.GRU(input_size=feature_dim, hidden_size=hidden_dim, num_layers=config[\"num_layers\"], \n","                                  dropout=config[\"dropout\"], batch_first=True, bias=True, bidirectional=True)\n","        self.relu = torch.nn.ReLU()\n","        self.linear = torch.nn.Linear(in_features=hidden_dim*2, out_features=output_dim, bias=True)\n","\n","    def forward(self, x):\n","        x, h = self.lstm(x)\n","        x = x[:, -1, :]\n","        x = self.relu(x)\n","        x = self.linear(x)\n","        return x"],"metadata":{"id":"ijcGL-bh_FFZ","executionInfo":{"status":"ok","timestamp":1671074655705,"user_tz":300,"elapsed":4,"user":{"displayName":"Alex Wang","userId":"17805231019310844218"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["# sanity check\n","torch.cuda.empty_cache()\n","\n","model = Model(config['feature_dim'], config['hidden_dim'], config['output_dim']).to(device)\n","prediction = model(input.to(device))\n","print(f'shape of prediction : {prediction.shape}')\n","print(f'shape of label : {label.shape}')\n","\n","summary(model, input.to(device))"],"metadata":{"id":"axcH-5N2F3B8","colab":{"base_uri":"https://localhost:8080/","height":498},"executionInfo":{"status":"ok","timestamp":1671074655705,"user_tz":300,"elapsed":4,"user":{"displayName":"Alex Wang","userId":"17805231019310844218"}},"outputId":"d441020e-8128-40d4-ea0b-b79c1742b1a9"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["shape of prediction : torch.Size([32, 1])\n","shape of label : torch.Size([32, 1])\n","=========================================================\n","         Kernel Shape   Output Shape   Params Mult-Adds\n","Layer                                                  \n","0_lstm              -  [32, 60, 128]  176.64k  174.336k\n","1_relu              -      [32, 128]        -         -\n","2_linear     [128, 1]        [32, 1]    129.0     128.0\n","---------------------------------------------------------\n","                        Totals\n","Total params          176.769k\n","Trainable params      176.769k\n","Non-trainable params       0.0\n","Mult-Adds             174.464k\n","=========================================================\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n","  df_sum = df.sum()\n"]},{"output_type":"execute_result","data":{"text/plain":["         Kernel Shape   Output Shape    Params  Mult-Adds\n","Layer                                                    \n","0_lstm              -  [32, 60, 128]  176640.0   174336.0\n","1_relu              -      [32, 128]       NaN        NaN\n","2_linear     [128, 1]        [32, 1]     129.0      128.0"],"text/html":["\n","  <div id=\"df-aff7f4fe-8618-4a6a-b516-1ad2a3690908\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Kernel Shape</th>\n","      <th>Output Shape</th>\n","      <th>Params</th>\n","      <th>Mult-Adds</th>\n","    </tr>\n","    <tr>\n","      <th>Layer</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0_lstm</th>\n","      <td>-</td>\n","      <td>[32, 60, 128]</td>\n","      <td>176640.0</td>\n","      <td>174336.0</td>\n","    </tr>\n","    <tr>\n","      <th>1_relu</th>\n","      <td>-</td>\n","      <td>[32, 128]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2_linear</th>\n","      <td>[128, 1]</td>\n","      <td>[32, 1]</td>\n","      <td>129.0</td>\n","      <td>128.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aff7f4fe-8618-4a6a-b516-1ad2a3690908')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-aff7f4fe-8618-4a6a-b516-1ad2a3690908 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-aff7f4fe-8618-4a6a-b516-1ad2a3690908');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["# Prepare loader\n","train_size = int(len(dataset) * 0.5)\n","val_size = int(len(dataset) * 0.2)\n","test_size = len(dataset) - train_size - val_size\n","train_data, val_data, test_data = dataset[0:train_size,:], dataset[train_size:train_size+val_size,:], dataset[train_size+val_size:len(dataset),:]\n","train_vol_10, val_vol_10, test_vol_10 = vol_10[0:train_size,:], vol_10[train_size:train_size+val_size,:], vol_10[train_size+val_size:len(dataset),:]\n","\n","train_loader = DataLoaderForVolatilityModeling(train_data, train_vol_10, batch_size=config['batch_size'], sequence_length=config['look_back'], shuffle=True)\n","val_loader = DataLoaderForVolatilityModeling(val_data, val_vol_10, batch_size=10, sequence_length=config['look_back'], shuffle=False)\n","test_loader = DataLoaderForVolatilityModeling(test_data, test_vol_10, batch_size=1, sequence_length=config['look_back'], shuffle=False)\n","\n","# Prepare optimizer, criterion, and scheduler_lr\n","optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay = 5e-5) # What goes in here?\n","criterion = torch.nn.MSELoss()\n","scheduler_lr = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.75, patience=2, verbose=True)"],"metadata":{"id":"o_66gyjnciwC","executionInfo":{"status":"ok","timestamp":1671074655857,"user_tz":300,"elapsed":155,"user":{"displayName":"Alex Wang","userId":"17805231019310844218"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["# Evaluate\n","def evaluate(val_loader, model, criterion):\n","    model.eval()\n","\n","    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc=\"Val\")\n","\n","    val_loss = 0\n","\n","    for i, (input, target) in enumerate(val_loader):\n","\n","        input, target = input.to(device), target.to(device)\n","\n","        with torch.inference_mode():\n","            prediction = model(input)\n","\n","        loss = criterion(prediction.flatten(), target.flatten())\n","\n","        val_loss += loss.item()\n","        # print(loss.item())\n","\n","        batch_bar.set_postfix(\n","            loss = f\"{val_loss/ (i+1):.4f}\",\n","            lr = f\"{curr_lr}\"\n","        )\n","\n","        batch_bar.update()\n","\n","        torch.cuda.empty_cache()\n","        del input\n","        del target\n","\n","    batch_bar.close()\n","    val_loss /= len(dataloader)\n","\n","    return val_loss"],"metadata":{"id":"FSffmC6qkVeA","executionInfo":{"status":"ok","timestamp":1671074655857,"user_tz":300,"elapsed":4,"user":{"displayName":"Alex Wang","userId":"17805231019310844218"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["# Train Step\n","def train_step(train_loader, model, optimizer, criterion):\n","    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n","    train_loss = 0\n","    model.train()\n","\n","    for i, (input, target) in enumerate(train_loader):\n","        input, target = input.to(device), target.to(device)\n","        curr_lr = optimizer.param_groups[0]['lr']\n","\n","        optimizer.zero_grad()\n","        prediction = model(input)\n","        # print(f'prediction : {prediction}')\n","        # print(f'target : {target}')\n","        loss = criterion(prediction.flatten(), target.flatten())\n","        # print(f'loss : {loss.item()}')\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","\n","        batch_bar.set_postfix(\n","            loss = f\"{train_loss/ (i+1):.4f}\",\n","            lr = f\"{curr_lr}\"\n","        )\n","\n","        batch_bar.update()\n","\n","        torch.cuda.empty_cache()\n","        del input\n","        del target\n","    \n","    batch_bar.close()\n","    train_loss /= len(train_loader)\n","    \n","    return train_loss"],"metadata":{"id":"7KZXr0DaqJh7","executionInfo":{"status":"ok","timestamp":1671074655858,"user_tz":300,"elapsed":5,"user":{"displayName":"Alex Wang","userId":"17805231019310844218"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["Y = vol_10[:-31, :]\n","\n","n_splits=10\n","\n","kf = KFold(n_splits)\n","kf.get_n_splits(Y)\n","\n","losses = []\n","\n","for i, (train_index, val_index) in enumerate(kf.split(Y)):\n","    train_X = np.take(dataset, train_index, axis=0)\n","    train_Y = np.reshape(np.take(Y, train_index), (-1, 1))\n","    val_X = np.take(dataset, val_index, axis=0)\n","    val_Y = np.reshape(np.take(Y, val_index), (-1, 1))\n","    train_loader = DataLoaderForVolatilityModeling(train_X, train_Y, batch_size=config['batch_size'], sequence_length=config['look_back'], shuffle=True)\n","    val_loader = DataLoaderForVolatilityModeling(val_X, val_Y, batch_size=10, sequence_length=config['look_back'], shuffle=False)\n","    \n","    model = Model(config['feature_dim'], config['hidden_dim'], config['output_dim']).to(device)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay = 5e-5) # What goes in here?\n","    criterion = torch.nn.MSELoss()\n","    scheduler_lr = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.75, patience=2, verbose=True)\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    best_val_loss = 100\n","    train_loss, val_loss = 0, 0\n","\n","    # Initialize wandb\n","    run = wandb.init(\n","        name = \"GRU_mult_feats_fold{:d}/{:d}\".format(i+1, n_splits), ## Wandb creates random run names if you skip this field\n","        reinit = True, ### Allows reinitalizing runs when you re-run this cell\n","        # run_id = ### Insert specific run id here if you want to resume a previous run\n","        # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n","        project = \"10701_Volatility_Prediction\", ### Project should be created in your wandb account \n","        config = config ### Wandb Config for your run\n","    )\n","\n","    for epoch in range(config[\"epochs\"]):\n","\n","        \n","        curr_lr = optimizer.param_groups[0]['lr']\n","\n","        train_loss = train_step(train_loader, model, optimizer, criterion)\n","\n","        val_loss = evaluate(val_loader, model, criterion)\n","\n","        scheduler_lr.step(train_loss)\n","\n","        print(\"\\nEpoch {}/{}: \\n\\t Train Loss {:.07f}\\t Eval Loss {:.07f}\\t Learning Rate {:.04f}\\t\".format(\n","              epoch + 1,\n","              config['epochs'],\n","              train_loss,\n","              val_loss,\n","              curr_lr))\n","        \n","        wandb.log({\"train_loss\":train_loss, 'val_loss': val_loss, \"learning_Rate\": curr_lr})\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","\n","    losses.append(best_val_loss)\n","\n","    run.finish()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["e5e93d1c22834ef7a8debe704f0c18ab","4f0c46f1ee61417291df8225b5d57f26","2147f7d7f1644f3281d41d64836f1d3e","862c01e53c44484884028bbcc473df45","9b72dbf0c0534edbb5086e65096921e0","14ffbf86eafa4c57a9121df150e8ba7b","f442e88b54574c99825e5ac82e96ea9d","48a685b2698540689d748a80cdd4ef72"]},"id":"gUnmHxHGlcyT","executionInfo":{"status":"error","timestamp":1671074952900,"user_tz":300,"elapsed":290417,"user":{"displayName":"Alex Wang","userId":"17805231019310844218"}},"outputId":"1d9ecd2a-0117-4d2c-a131-70b13a11909c"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stderr","text":["Exception ignored in: <function WeakSet.__init__.<locals>._remove at 0x7f35fe825670>\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.8/_weakrefset.py\", line 38, in _remove\n","    def _remove(item, selfref=ref(self)):\n","KeyboardInterrupt: \n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Finishing last run (ID:2z001vud) before initializing another..."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5e93d1c22834ef7a8debe704f0c18ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Synced <strong style=\"color:#cdcd00\">LSTM_mult_feats_6</strong>: <a href=\"https://wandb.ai/quantoalpha/10701_Volatility_Prediction/runs/2z001vud\" target=\"_blank\">https://wandb.ai/quantoalpha/10701_Volatility_Prediction/runs/2z001vud</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20221215_032415-2z001vud/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Successfully finished last run (ID:2z001vud). Initializing new run:<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.13.7"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20221215_032422-3epmtwp2</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/quantoalpha/10701_Volatility_Prediction/runs/3epmtwp2\" target=\"_blank\">GRU_mult_feats_fold1/10</a></strong> to <a href=\"https://wandb.ai/quantoalpha/10701_Volatility_Prediction\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1/100: \n","\t Train Loss 0.2279211\t Eval Loss 0.0073330\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 2/100: \n","\t Train Loss 0.0759581\t Eval Loss 0.0053578\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 3/100: \n","\t Train Loss 0.0592404\t Eval Loss 0.0035148\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 4/100: \n","\t Train Loss 0.0543657\t Eval Loss 0.0027407\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 5/100: \n","\t Train Loss 0.0490166\t Eval Loss 0.0043704\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 6/100: \n","\t Train Loss 0.0454369\t Eval Loss 0.0030389\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 7/100: \n","\t Train Loss 0.0415275\t Eval Loss 0.0030345\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 8/100: \n","\t Train Loss 0.0351493\t Eval Loss 0.0025785\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 9/100: \n","\t Train Loss 0.0271090\t Eval Loss 0.0019026\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 10/100: \n","\t Train Loss 0.0233803\t Eval Loss 0.0012673\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 11/100: \n","\t Train Loss 0.0200644\t Eval Loss 0.0010954\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 12/100: \n","\t Train Loss 0.0186124\t Eval Loss 0.0010870\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 13/100: \n","\t Train Loss 0.0182286\t Eval Loss 0.0009073\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 14/100: \n","\t Train Loss 0.0164638\t Eval Loss 0.0008413\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 15/100: \n","\t Train Loss 0.0171300\t Eval Loss 0.0007847\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 16/100: \n","\t Train Loss 0.0150826\t Eval Loss 0.0010461\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 17/100: \n","\t Train Loss 0.0166718\t Eval Loss 0.0007499\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 18/100: \n","\t Train Loss 0.0133091\t Eval Loss 0.0007856\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 19/100: \n","\t Train Loss 0.0142521\t Eval Loss 0.0008993\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 20/100: \n","\t Train Loss 0.0135400\t Eval Loss 0.0011160\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 21/100: \n","\t Train Loss 0.0122984\t Eval Loss 0.0007469\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 22/100: \n","\t Train Loss 0.0123530\t Eval Loss 0.0006969\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 23/100: \n","\t Train Loss 0.0129393\t Eval Loss 0.0011862\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 00024: reducing learning rate of group 0 to 7.5000e-04.\n","\n","Epoch 24/100: \n","\t Train Loss 0.0133236\t Eval Loss 0.0011166\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 25/100: \n","\t Train Loss 0.0117492\t Eval Loss 0.0007901\t Learning Rate 0.0008\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 26/100: \n","\t Train Loss 0.0115342\t Eval Loss 0.0007755\t Learning Rate 0.0008\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 27/100: \n","\t Train Loss 0.0114419\t Eval Loss 0.0007440\t Learning Rate 0.0008\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 28/100: \n","\t Train Loss 0.0106694\t Eval Loss 0.0008333\t Learning Rate 0.0008\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 29/100: \n","\t Train Loss 0.0103335\t Eval Loss 0.0008726\t Learning Rate 0.0008\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 30/100: \n","\t Train Loss 0.0103255\t Eval Loss 0.0008504\t Learning Rate 0.0008\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 31/100: \n","\t Train Loss 0.0100454\t Eval Loss 0.0013447\t Learning Rate 0.0008\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 32/100: \n","\t Train Loss 0.0098648\t Eval Loss 0.0009798\t Learning Rate 0.0008\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 33/100: \n","\t Train Loss 0.0101270\t Eval Loss 0.0009231\t Learning Rate 0.0008\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 34/100: \n","\t Train Loss 0.0100418\t Eval Loss 0.0009289\t Learning Rate 0.0008\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 00035: reducing learning rate of group 0 to 5.6250e-04.\n","\n","Epoch 35/100: \n","\t Train Loss 0.0105803\t Eval Loss 0.0010207\t Learning Rate 0.0008\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 36/100: \n","\t Train Loss 0.0106314\t Eval Loss 0.0005731\t Learning Rate 0.0006\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 37/100: \n","\t Train Loss 0.0095166\t Eval Loss 0.0004985\t Learning Rate 0.0006\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 38/100: \n","\t Train Loss 0.0096287\t Eval Loss 0.0004684\t Learning Rate 0.0006\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 39/100: \n","\t Train Loss 0.0095256\t Eval Loss 0.0005287\t Learning Rate 0.0006\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 00040: reducing learning rate of group 0 to 4.2188e-04.\n","\n","Epoch 40/100: \n","\t Train Loss 0.0095905\t Eval Loss 0.0005743\t Learning Rate 0.0006\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 41/100: \n","\t Train Loss 0.0089392\t Eval Loss 0.0004862\t Learning Rate 0.0004\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 42/100: \n","\t Train Loss 0.0090690\t Eval Loss 0.0004759\t Learning Rate 0.0004\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 43/100: \n","\t Train Loss 0.0091484\t Eval Loss 0.0005100\t Learning Rate 0.0004\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 44/100: \n","\t Train Loss 0.0087742\t Eval Loss 0.0005053\t Learning Rate 0.0004\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 45/100: \n","\t Train Loss 0.0086754\t Eval Loss 0.0005189\t Learning Rate 0.0004\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 46/100: \n","\t Train Loss 0.0087818\t Eval Loss 0.0007080\t Learning Rate 0.0004\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 47/100: \n","\t Train Loss 0.0085927\t Eval Loss 0.0004315\t Learning Rate 0.0004\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 48/100: \n","\t Train Loss 0.0083630\t Eval Loss 0.0005235\t Learning Rate 0.0004\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 49/100: \n","\t Train Loss 0.0085182\t Eval Loss 0.0005073\t Learning Rate 0.0004\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 50/100: \n","\t Train Loss 0.0084476\t Eval Loss 0.0004617\t Learning Rate 0.0004\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 00051: reducing learning rate of group 0 to 3.1641e-04.\n","\n","Epoch 51/100: \n","\t Train Loss 0.0085555\t Eval Loss 0.0004932\t Learning Rate 0.0004\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 52/100: \n","\t Train Loss 0.0078606\t Eval Loss 0.0004517\t Learning Rate 0.0003\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 53/100: \n","\t Train Loss 0.0082107\t Eval Loss 0.0004808\t Learning Rate 0.0003\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 54/100: \n","\t Train Loss 0.0081440\t Eval Loss 0.0004345\t Learning Rate 0.0003\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 00055: reducing learning rate of group 0 to 2.3730e-04.\n","\n","Epoch 55/100: \n","\t Train Loss 0.0078900\t Eval Loss 0.0004597\t Learning Rate 0.0003\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 56/100: \n","\t Train Loss 0.0076999\t Eval Loss 0.0004558\t Learning Rate 0.0002\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 57/100: \n","\t Train Loss 0.0078513\t Eval Loss 0.0004781\t Learning Rate 0.0002\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 58/100: \n","\t Train Loss 0.0077975\t Eval Loss 0.0004503\t Learning Rate 0.0002\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 00059: reducing learning rate of group 0 to 1.7798e-04.\n","\n","Epoch 59/100: \n","\t Train Loss 0.0079108\t Eval Loss 0.0004112\t Learning Rate 0.0002\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 60/100: \n","\t Train Loss 0.0075999\t Eval Loss 0.0004550\t Learning Rate 0.0002\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 61/100: \n","\t Train Loss 0.0074914\t Eval Loss 0.0004281\t Learning Rate 0.0002\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 62/100: \n","\t Train Loss 0.0075594\t Eval Loss 0.0004432\t Learning Rate 0.0002\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 63/100: \n","\t Train Loss 0.0075419\t Eval Loss 0.0004522\t Learning Rate 0.0002\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 00064: reducing learning rate of group 0 to 1.3348e-04.\n","\n","Epoch 64/100: \n","\t Train Loss 0.0075743\t Eval Loss 0.0004468\t Learning Rate 0.0002\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 65/100: \n","\t Train Loss 0.0074192\t Eval Loss 0.0004301\t Learning Rate 0.0001\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 66/100: \n","\t Train Loss 0.0075449\t Eval Loss 0.0004195\t Learning Rate 0.0001\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 67/100: \n","\t Train Loss 0.0074123\t Eval Loss 0.0004331\t Learning Rate 0.0001\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 68/100: \n","\t Train Loss 0.0069989\t Eval Loss 0.0004299\t Learning Rate 0.0001\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 69/100: \n","\t Train Loss 0.0071979\t Eval Loss 0.0004208\t Learning Rate 0.0001\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 70/100: \n","\t Train Loss 0.0071620\t Eval Loss 0.0004343\t Learning Rate 0.0001\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 00071: reducing learning rate of group 0 to 1.0011e-04.\n","\n","Epoch 71/100: \n","\t Train Loss 0.0072478\t Eval Loss 0.0004375\t Learning Rate 0.0001\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 72/100: \n","\t Train Loss 0.0073708\t Eval Loss 0.0004031\t Learning Rate 0.0001\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 73/100: \n","\t Train Loss 0.0074118\t Eval Loss 0.0004581\t Learning Rate 0.0001\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 00074: reducing learning rate of group 0 to 7.5085e-05.\n","\n","Epoch 74/100: \n","\t Train Loss 0.0072046\t Eval Loss 0.0004176\t Learning Rate 0.0001\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 75/100: \n","\t Train Loss 0.0072970\t Eval Loss 0.0004019\t Learning Rate 0.0001\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 76/100: \n","\t Train Loss 0.0067767\t Eval Loss 0.0003985\t Learning Rate 0.0001\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 77/100: \n","\t Train Loss 0.0068309\t Eval Loss 0.0004022\t Learning Rate 0.0001\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 78/100: \n","\t Train Loss 0.0070690\t Eval Loss 0.0003969\t Learning Rate 0.0001\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 00079: reducing learning rate of group 0 to 5.6314e-05.\n","\n","Epoch 79/100: \n","\t Train Loss 0.0073139\t Eval Loss 0.0004118\t Learning Rate 0.0001\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 80/100: \n","\t Train Loss 0.0068735\t Eval Loss 0.0003960\t Learning Rate 0.0001\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 81/100: \n","\t Train Loss 0.0067273\t Eval Loss 0.0004078\t Learning Rate 0.0001\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 82/100: \n","\t Train Loss 0.0069369\t Eval Loss 0.0003986\t Learning Rate 0.0001\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 83/100: \n","\t Train Loss 0.0071455\t Eval Loss 0.0004012\t Learning Rate 0.0001\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 00084: reducing learning rate of group 0 to 4.2235e-05.\n","\n","Epoch 84/100: \n","\t Train Loss 0.0068640\t Eval Loss 0.0004066\t Learning Rate 0.0001\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 85/100: \n","\t Train Loss 0.0069786\t Eval Loss 0.0003994\t Learning Rate 0.0000\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 86/100: \n","\t Train Loss 0.0068302\t Eval Loss 0.0003906\t Learning Rate 0.0000\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 87/100: \n","\t Train Loss 0.0066573\t Eval Loss 0.0004016\t Learning Rate 0.0000\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 88/100: \n","\t Train Loss 0.0068643\t Eval Loss 0.0003893\t Learning Rate 0.0000\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 89/100: \n","\t Train Loss 0.0069191\t Eval Loss 0.0003907\t Learning Rate 0.0000\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 00090: reducing learning rate of group 0 to 3.1676e-05.\n","\n","Epoch 90/100: \n","\t Train Loss 0.0072446\t Eval Loss 0.0003966\t Learning Rate 0.0000\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 91/100: \n","\t Train Loss 0.0067519\t Eval Loss 0.0003951\t Learning Rate 0.0000\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 92/100: \n","\t Train Loss 0.0068268\t Eval Loss 0.0003912\t Learning Rate 0.0000\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 93/100: \n","\t Train Loss 0.0066367\t Eval Loss 0.0004071\t Learning Rate 0.0000\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 94/100: \n","\t Train Loss 0.0069727\t Eval Loss 0.0003924\t Learning Rate 0.0000\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 95/100: \n","\t Train Loss 0.0066972\t Eval Loss 0.0003860\t Learning Rate 0.0000\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 00096: reducing learning rate of group 0 to 2.3757e-05.\n","\n","Epoch 96/100: \n","\t Train Loss 0.0067890\t Eval Loss 0.0003994\t Learning Rate 0.0000\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 97/100: \n","\t Train Loss 0.0068517\t Eval Loss 0.0003944\t Learning Rate 0.0000\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 98/100: \n","\t Train Loss 0.0067878\t Eval Loss 0.0004026\t Learning Rate 0.0000\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 00099: reducing learning rate of group 0 to 1.7818e-05.\n","\n","Epoch 99/100: \n","\t Train Loss 0.0068238\t Eval Loss 0.0003926\t Learning Rate 0.0000\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 100/100: \n","\t Train Loss 0.0070276\t Eval Loss 0.0003946\t Learning Rate 0.0000\t\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learning_Rate</td><td>██████████▆▆▆▆▅▅▄▄▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▄▃▂▂▂▁▁▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learning_Rate</td><td>2e-05</td></tr><tr><td>train_loss</td><td>0.00703</td></tr><tr><td>val_loss</td><td>0.00039</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Synced <strong style=\"color:#cdcd00\">GRU_mult_feats_fold1/10</strong>: <a href=\"https://wandb.ai/quantoalpha/10701_Volatility_Prediction/runs/3epmtwp2\" target=\"_blank\">https://wandb.ai/quantoalpha/10701_Volatility_Prediction/runs/3epmtwp2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20221215_032422-3epmtwp2/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.13.7"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20221215_032842-3af7gypy</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/quantoalpha/10701_Volatility_Prediction/runs/3af7gypy\" target=\"_blank\">GRU_mult_feats_fold2/10</a></strong> to <a href=\"https://wandb.ai/quantoalpha/10701_Volatility_Prediction\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1/100: \n","\t Train Loss 0.1961828\t Eval Loss 0.0023121\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 2/100: \n","\t Train Loss 0.0708250\t Eval Loss 0.0020126\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 3/100: \n","\t Train Loss 0.0556617\t Eval Loss 0.0014681\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 4/100: \n","\t Train Loss 0.0490946\t Eval Loss 0.0013689\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 5/100: \n","\t Train Loss 0.0444247\t Eval Loss 0.0018074\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 6/100: \n","\t Train Loss 0.0351083\t Eval Loss 0.0014203\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 7/100: \n","\t Train Loss 0.0284489\t Eval Loss 0.0011097\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 8/100: \n","\t Train Loss 0.0225460\t Eval Loss 0.0009564\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 9/100: \n","\t Train Loss 0.0220455\t Eval Loss 0.0010066\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 10/100: \n","\t Train Loss 0.0201826\t Eval Loss 0.0007477\t Learning Rate 0.0010\t\n"]},{"output_type":"stream","name":"stderr","text":["Train:  62%|██████▏   | 107/172 [00:01<00:01, 64.58it/s, loss=0.0150, lr=0.001]"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-67-61213776eac2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mcurr_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-65-b74a35660169>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(train_loader, model, optimizer, criterion)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         batch_bar.set_postfix(\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{train_loss/ (i+1):.4f}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{curr_lr}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mset_postfix\u001b[0;34m(self, ordered_dict, refresh, **kwargs)\u001b[0m\n\u001b[1;32m   1443\u001b[0m                                  for key in postfix.keys())\n\u001b[1;32m   1444\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1445\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_postfix_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1509\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mprint_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0mlen_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisp_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m             \u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mfp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0mfp_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mlast_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# request flush on the background thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m             \u001b[0;31m# wait for flush to actually get through, if we can.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;31m# waiting across threads during import can cause deadlocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    616\u001b[0m                 )\n\u001b[1;32m    617\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m     def send_multipart(\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["print(losses)"],"metadata":{"id":"umtg5UQYzu6R","executionInfo":{"status":"aborted","timestamp":1671074952901,"user_tz":300,"elapsed":10,"user":{"displayName":"Alex Wang","userId":"17805231019310844218"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}\n","\n","for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n","\n","    print('Fold {}'.format(fold + 1))\n","\n","    train_sampler = SubsetRandomSampler(train_idx)\n","    test_sampler = SubsetRandomSampler(val_idx)\n","    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n","    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n","    \n","    model = Model(config['feature_dim'], config['hidden_dim'], config['output_dim']).to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=0.002)\n","\n","    for epoch in range(num_epochs):\n","        train_loss, train_correct=train_epoch(model,device,train_loader,criterion,optimizer)\n","        test_loss, test_correct=valid_epoch(model,device,test_loader,criterion)\n","\n","        train_loss = train_loss / len(train_loader.sampler)\n","        train_acc = train_correct / len(train_loader.sampler) * 100\n","        test_loss = test_loss / len(test_loader.sampler)\n","        test_acc = test_correct / len(test_loader.sampler) * 100\n","\n","        print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %\".format(epoch + 1,\n","                                                                                                             num_epochs,\n","                                                                                                             train_loss,\n","                                                                                                             test_loss,\n","                                                                                                             train_acc,\n","                                                                                                             test_acc))\n","        history['train_loss'].append(train_loss)\n","        history['test_loss'].append(test_loss)\n","        history['train_acc'].append(train_acc)\n","        history['test_acc'].append(test_acc)   \n"],"metadata":{"id":"7jFxKIj9kXEa","executionInfo":{"status":"aborted","timestamp":1671074952902,"user_tz":300,"elapsed":11,"user":{"displayName":"Alex Wang","userId":"17805231019310844218"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_val_loss = 100\n","\n","# Train loop\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","train_loss, val_loss = 0, 0\n","\n","for epoch in range(config[\"epochs\"]):\n","    \n","    curr_lr = optimizer.param_groups[0]['lr']\n","\n","    train_loss = train_step(train_loader, model, optimizer, criterion)\n","\n","    val_loss = evaluate(val_loader, model, criterion)\n","\n","    scheduler_lr.step(train_loss)\n","\n","    print(\"\\nEpoch {}/{}: \\n\\t Train Loss {:.07f}\\t Eval Loss {:.07f}\\t Learning Rate {:.04f}\\t\".format(\n","          epoch + 1,\n","          config['epochs'],\n","          train_loss,\n","          val_loss,\n","          curr_lr))\n","    \n","    wandb.log({\"train_loss\":train_loss, 'val_loss': val_loss, \"learning_Rate\": curr_lr})\n","\n","    if val_loss < best_val_loss:\n","        path = '/content/drive/MyDrive/10701/checkpoint/checkpoint_eval_loss_haha'.format(val_loss)\n","        torch.save({'model_state_dict': model.state_dict()}, path)\n","        best_val_loss = val_loss\n","\n","wandb.save(path)\n","run.finish()"],"metadata":{"id":"uTQUQq2DucIY","executionInfo":{"status":"aborted","timestamp":1671074952902,"user_tz":300,"elapsed":11,"user":{"displayName":"Alex Wang","userId":"17805231019310844218"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoint = torch.load(path)\n","model.load_state_dict(checkpoint['model_state_dict'])"],"metadata":{"id":"PQDxXjrxyxbN","executionInfo":{"status":"aborted","timestamp":1671074952902,"user_tz":300,"elapsed":11,"user":{"displayName":"Alex Wang","userId":"17805231019310844218"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prediction\n","def predict(test_loader, model):\n","    model.eval()\n","    preds, targets = [], []\n","    test_loss = 0\n","\n","    for i, (input, target) in enumerate(test_loader):\n","\n","        input, target = input.to(device), target.to(device)\n","\n","        with torch.inference_mode():\n","            prediction = model(input)\n","\n","        test_loss += criterion(prediction.flatten(), target.flatten())\n","\n","        # Note: Each loaded data is a column\n","        prediction = prediction.cpu().detach().numpy() # (batch_size, feature_size)\n","        target = target.cpu().detach().numpy() # (batch_size, feature_size)\n","\n","        preds.append(prediction.flatten()[0])\n","        targets.append(target.flatten()[0])\n","\n","        del input\n","        del target\n","    \n","    test_loss /= len(test_loader)\n","\n","    return preds, targets, test_loss"],"metadata":{"id":"G-esSD0YxWDO","executionInfo":{"status":"aborted","timestamp":1671074952903,"user_tz":300,"elapsed":12,"user":{"displayName":"Alex Wang","userId":"17805231019310844218"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.empty_cache()\n","gc.collect()\n","\n","preds, targets, test_loss = predict(test_loader, model)\n","\n","loss = 'Eval loss : {:.07}'.format(best_val_loss)\n","print(loss)\n","\n","loss = 'Test loss : {:.07}'.format(test_loss)\n","print(loss)"],"metadata":{"id":"QJ8_E3dexBRF","executionInfo":{"status":"aborted","timestamp":1671074952903,"user_tz":300,"elapsed":12,"user":{"displayName":"Alex Wang","userId":"17805231019310844218"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mean absolute percentage error\n","MAPE = 0\n","for i in range(len(preds)):\n","    MAPE += np.abs((targets[i]-preds[i])/targets[i])\n","\n","print(MAPE/len(preds))"],"metadata":{"id":"fVGigLrYqyIb","executionInfo":{"status":"aborted","timestamp":1671074952904,"user_tz":300,"elapsed":12,"user":{"displayName":"Alex Wang","userId":"17805231019310844218"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = data.copy()\n","df = df.tail(len(preds))\n","df.drop(df.columns, inplace=True, axis=1)\n","df['preds'] = preds\n","\n","# Generate output\n","plt.figure(figsize=(13,9))\n","plt.plot(df.index, preds, label=\"Prediction\", color=\"red\")\n","plt.plot(df.index, targets, label=\"Targets\", color=\"blue\")\n","plt.xlabel(\"Years\")\n","plt.ylabel(\"10-Day Volatility\")\n","plt.legend(loc=\"upper left\")\n","plt.show()"],"metadata":{"id":"3M1UTQSoy6JQ","executionInfo":{"status":"aborted","timestamp":1671074952904,"user_tz":300,"elapsed":12,"user":{"displayName":"Alex Wang","userId":"17805231019310844218"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prediction for the entire dataset\n","data_loader = DataLoaderForVolatilityModeling(dataset, vol_10,batch_size=1, sequence_length=config['look_back'], shuffle=False)\n","\n","# Make predictions\n","preds, targets, test_loss = predict(data_loader, model)\n","\n","# Generate output\n","plt.figure(figsize=(13,9))\n","plt.plot(data.index[:len(preds)], preds, label=\"Prediction\", color=\"red\")\n","plt.plot(data.index[:len(targets)], targets, label=\"Targets\", color=\"blue\")\n","plt.xlabel(\"Years\")\n","plt.ylabel(\"10-Day Volatility\")\n","plt.legend(loc=\"upper left\")\n","plt.show()\n","\n","loss = 'Test loss : {:.07}'.format(test_loss)\n","print(loss)"],"metadata":{"id":"ICJJcAITfPab","executionInfo":{"status":"aborted","timestamp":1671074952904,"user_tz":300,"elapsed":12,"user":{"displayName":"Alex Wang","userId":"17805231019310844218"}}},"execution_count":null,"outputs":[]}]}